{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DSCI 5260- SECTION 001\n",
        "BUSINESS PROCESS ANALYTICS**"
      ],
      "metadata": {
        "id": "alLja_KA8CYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group 6- Project Milestone Report**"
      ],
      "metadata": {
        "id": "L_TbayEc8fgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries**"
      ],
      "metadata": {
        "id": "eyV50H4W8qAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "dQoj2SrYJwRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling missing values**"
      ],
      "metadata": {
        "id": "8ZTqsFnH8tr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/TaxiTripDataDecmeber2023.csv')\n",
        "\n",
        "# Display first five rows\n",
        "print(\"First five rows:\")\n",
        "print(df.head(5))\n",
        "\n",
        "# Show missing values per column\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
        "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "# Display missing values\n",
        "print(\"\\nMissing Values Summary:\")\n",
        "print(missing_df)\n",
        "\n",
        "# Drop columns with excessive missing values (>80%)\n",
        "cols_to_drop = missing_df[missing_df['Percentage'] > 80].index.tolist()\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "print(f\"\\nDropped columns due to excessive missing values: {cols_to_drop}\")\n",
        "\n",
        "# Fill missing values\n",
        "df_filled = df.copy()\n",
        "\n",
        "# Convert datetime columns\n",
        "df_filled['ORIGINDATETIME_TR'] = pd.to_datetime(df_filled['ORIGINDATETIME_TR'], errors='coerce')\n",
        "df_filled['DESTINATIONDATETIME_TR'] = pd.to_datetime(df_filled['DESTINATIONDATETIME_TR'], errors='coerce')\n",
        "\n",
        "# Feature Engineering: Calculate trip duration (minutes)\n",
        "df_filled['trip_duration'] = (df_filled['DESTINATIONDATETIME_TR'] - df_filled['ORIGINDATETIME_TR']).dt.total_seconds() / 60\n",
        "\n",
        "# Fill missing numeric values\n",
        "numeric_cols = df_filled.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in numeric_cols:\n",
        "    if df_filled[col].isnull().sum() > 0:\n",
        "        if df_filled[col].skew() > 1:  # If skewed, use median\n",
        "            df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
        "        else:  # Otherwise, use mean\n",
        "            df_filled[col].fillna(df_filled[col].mean(), inplace=True)\n",
        "\n",
        "# Fill categorical missing values with mode\n",
        "categorical_cols = df_filled.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    if df_filled[col].isnull().sum() > 0:\n",
        "        df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\nMissing values handled successfully!\")"
      ],
      "metadata": {
        "id": "fGjPiDHQh90p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.head(5)"
      ],
      "metadata": {
        "id": "oTTFOSCOJzqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding unique values in ORIGINCITY, to remove unwanted characters**"
      ],
      "metadata": {
        "id": "iSyG2HEu9Ogn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled['ORIGINCITY'].unique()"
      ],
      "metadata": {
        "id": "FXHAU0afKEdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing unwanted characters**"
      ],
      "metadata": {
        "id": "8mBoF7ds9bVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to title case and strip spaces\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].str.strip().str.title()\n",
        "\n",
        "# Replace invalid values with NaN\n",
        "invalid_values = [\"???\", \"-\", \"None\", \"Unknown\", \"Na\", \"106\", \"101\", \"Suite #203\"]\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].replace(invalid_values, np.nan)\n",
        "\n",
        "# Define common name corrections\n",
        "name_corrections = {\n",
        "    \"Washington Dc\": \"Washington\",\n",
        "    \"Mclean\": \"McLean\",\n",
        "    \"Oxon Hill Md\": \"Oxon Hill\",\n",
        "    \"District Heights Md\": \"District Heights\",\n",
        "    \"Falls Church Va\": \"Falls Church\",\n",
        "    \"Suitland-Silver Hill\": \"Suitland\",\n",
        "    \"New Carrollton Md\": \"New Carrollton\"\n",
        "}\n",
        "\n",
        "# Apply corrections\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].replace(name_corrections)\n",
        "\n",
        "# Remove rows with full addresses\n",
        "df_filled = df_filled[~df_filled['ORIGINCITY'].str.contains(r'\\d', na=False)]\n",
        "\n",
        "# Check the cleaned unique values\n",
        "print(df_filled['ORIGINCITY'].unique())"
      ],
      "metadata": {
        "id": "KV2pj99DK746"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['ORIGINCITY'] == \"UNKNOWN\").sum())"
      ],
      "metadata": {
        "id": "MV25haYiLXow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['ORIGINCITY'] == \"-\").sum())\n"
      ],
      "metadata": {
        "id": "lHHBfdfDLjC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['ORIGINSTATE'].str.strip() == \"NA\").sum())\n",
        "print(df_filled['ORIGINSTATE'].isna().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "KAZnLX6lLpB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding unique values in DESTINATIONSTATE, to remove unwanted characters**"
      ],
      "metadata": {
        "id": "dj1cZHcN9lyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['DESTINATIONSTATE'].unique())"
      ],
      "metadata": {
        "id": "80o26PYeLvuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing unwanted characters from DESTINATIONSTATE**"
      ],
      "metadata": {
        "id": "i7WuUUe99yBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of valid U.S. state abbreviations\n",
        "valid_states = {\n",
        "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
        "    'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
        "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT',\n",
        "    'VA', 'WA', 'WV', 'WI', 'WY', 'DC'\n",
        "}\n",
        "\n",
        "# Strip spaces\n",
        "df_filled['DESTINATIONSTATE'] = df['DESTINATIONSTATE'].str.strip()\n",
        "\n",
        "# Replace invalid values with NaN\n",
        "invalid_values = [\"- \", \"Na\", \"None\", \"Unknown\", \"\", \"nan\"]\n",
        "df_filled['DESTINATIONSTATE'] = df['DESTINATIONSTATE'].replace(invalid_values, np.nan)\n",
        "\n",
        "# Keep only valid state abbreviations\n",
        "df_filled['DESTINATIONSTATE'] = df_filled['DESTINATIONSTATE'].apply(lambda x: x if x in valid_states else np.nan)\n",
        "\n",
        "# Check cleaned unique values\n",
        "print(df_filled['DESTINATIONSTATE'].unique())"
      ],
      "metadata": {
        "id": "K9M1YNijLxtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding unique values in DESTINATIONCITY, to remove unwanted characters**"
      ],
      "metadata": {
        "id": "GN5_b-xn968A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['DESTINATIONCITY'].unique())"
      ],
      "metadata": {
        "id": "6JizT_BBL8HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing unwanted characters from DESTINATIONCITY**"
      ],
      "metadata": {
        "id": "GbZo5h5Y-AUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled['DESTINATIONCITY'] = df_filled['DESTINATIONCITY'].str.strip()  # Remove leading/trailing spaces\n",
        "df_filled['DESTINATIONCITY'] = df_filled['DESTINATIONCITY'].str.title()  # Standardize capitalization\n",
        "\n",
        "# Replace unwanted values\n",
        "invalid_values = [\"Unknown\", \"-\", \"???\", \"None\", \"UNKNOWN\"]\n",
        "df_filled['DESTINATIONCITY'] = df_filled['DESTINATIONCITY'].replace(invalid_values, \"Missing\")\n",
        "\n",
        "# Display unique values after cleaning\n",
        "print(df_filled['DESTINATIONCITY'].unique())"
      ],
      "metadata": {
        "id": "c1Ak5MnuMGwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['DESTINATIONCITY']== \"-\").sum())"
      ],
      "metadata": {
        "id": "MJ4zwnIbMXUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['DESTINATIONCITY']== \"UNKNOWN\").sum())"
      ],
      "metadata": {
        "id": "yPiLnulsMcR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['DESTINATIONCITY']== \"???\").sum())"
      ],
      "metadata": {
        "id": "PR61Xb1CM5oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding number of null values**"
      ],
      "metadata": {
        "id": "j2QfnBum-d43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['ORIGIN_BLOCK_LATITUDE',\n",
        "            'ORIGIN_BLOCK_LONGITUDE',\n",
        "            'ORIGIN_BLOCKNAME',\n",
        "            'DESTINATION_BLOCK_LAT',\n",
        "            'DESTINATION_BLOCK_LONG',\n",
        "            'DESTINATION_BLOCKNAME',\n",
        "            'AIRPORT']\n",
        "\n",
        "for column in columns:\n",
        "    print(f\"Unique values in {column}:\")\n",
        "    print(df_filled[column].unique())\n",
        "    print(df_filled[column].isna().sum())\n",
        "    print()"
      ],
      "metadata": {
        "id": "Izyvn_VSP1y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_filled['ORIGIN_BLOCKNAME'] == \"\").sum())"
      ],
      "metadata": {
        "id": "CUtYv9e4P_82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding unique values in DESTINATIONZIP, to remove unwanted characters**"
      ],
      "metadata": {
        "id": "WHR1y1Ze-k70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['DESTINATIONZIP'].unique())"
      ],
      "metadata": {
        "id": "r3ZZy2qESMzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing every value this is not 5 numerical character length**\n"
      ],
      "metadata": {
        "id": "E8ZjAFeC-6L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_zip(zip_code):\n",
        "    zip_code = str(zip_code).strip()  # Remove leading/trailing spaces\n",
        "    if zip_code.isdigit() and len(zip_code) == 5:  # Valid 5-digit ZIP\n",
        "        return zip_code\n",
        "    return None  # Drop invalid ZIP codes\n",
        "\n",
        "# Apply cleaning function\n",
        "df_filled['DESTINATIONZIP'] = df_filled['DESTINATIONZIP'].apply(clean_zip)"
      ],
      "metadata": {
        "id": "xdKklTGKSjtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['DESTINATIONZIP'].unique())"
      ],
      "metadata": {
        "id": "FDOb5CGvS6ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Unique values in ORIGINZIP to remove unwanted characters**"
      ],
      "metadata": {
        "id": "MPB-lPJ4_F8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['ORIGINZIP'].unique())"
      ],
      "metadata": {
        "id": "Cqf-1m22SOYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing every value this is not 5 numerical character length**"
      ],
      "metadata": {
        "id": "IS_hgUb3_QVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_zip(zip_code):\n",
        "    zip_code = str(zip_code).strip()  # Remove leading/trailing spaces\n",
        "    if zip_code.isdigit() and len(zip_code) == 5:  # Valid 5-digit ZIP\n",
        "        return zip_code\n",
        "    return None  # Drop invalid ZIP codes\n",
        "\n",
        "# Apply cleaning function\n",
        "df_filled['ORIGINZIP'] = df_filled['ORIGINZIP'].apply(clean_zip)"
      ],
      "metadata": {
        "id": "xMMUGo9JSS0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['ORIGINZIP'].unique())"
      ],
      "metadata": {
        "id": "86KaEeUqTOZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.head(5)"
      ],
      "metadata": {
        "id": "0Fsjc-tFTSSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_filled['DESTINATIONCITY'].value_counts())\n"
      ],
      "metadata": {
        "id": "9cOxOd8dTVBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding out if there is Missing as the value in DESTINATIONCITY column**"
      ],
      "metadata": {
        "id": "BB0aldjCAGAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.loc[df_filled['DESTINATIONCITY'] == 'Missing', ['DESTINATIONCITY']]\n"
      ],
      "metadata": {
        "id": "bYfT70AtUyr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.shape"
      ],
      "metadata": {
        "id": "NKS-tS8xVKsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing rows with missing as the value**"
      ],
      "metadata": {
        "id": "seCs1G0FAUTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_check = ['DESTINATIONCITY', 'DESTINATIONZIP', 'DESTINATIONSTATE', 'ORIGINZIP', 'ORIGINSTATE']\n",
        "\n",
        "# Remove rows with 'Missing' or empty values in the specified columns\n",
        "df_filled = df_filled[~df_filled[columns_to_check].isin(['Missing', '', 'None']).any(axis=1)]\n",
        "\n",
        "columns_to_check = ['DESTINATIONCITY', 'DESTINATIONZIP', 'DESTINATIONSTATE', 'ORIGINZIP', 'ORIGINSTATE']\n",
        "\n",
        "# Remove rows with 'Missing' or empty values in the specified columns\n",
        "df_filled = df_filled[~df_filled[columns_to_check].isin(['Missing', '']).any(axis=1)]"
      ],
      "metadata": {
        "id": "CkHmbX60VZlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.loc[df_filled['DESTINATIONCITY'] == 'Missing', ['DESTINATIONCITY']]"
      ],
      "metadata": {
        "id": "ytiOtn4rWW6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for NULL values**"
      ],
      "metadata": {
        "id": "5sBTTn7gAqJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.loc[df_filled[columns_to_check].eq('').any(axis=1)]\n"
      ],
      "metadata": {
        "id": "PozcyMGxWo3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.loc[df_filled['AIRPORT'].eq('')]\n"
      ],
      "metadata": {
        "id": "0mYNibnmWx4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for Unique values in AIRPORT column**"
      ],
      "metadata": {
        "id": "7K1nNOwmAv-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled['AIRPORT'].unique()\n"
      ],
      "metadata": {
        "id": "H79kfOG-Xi-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for NONE values in ORIGINZIP**"
      ],
      "metadata": {
        "id": "61q73wnRA-xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled['ORIGINZIP'].head(10)"
      ],
      "metadata": {
        "id": "mNsar-yZXr4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df_filled.dropna(subset=['ORIGINZIP'])\n",
        "df_filled = df_filled[~df_filled['ORIGINZIP'].eq('')]\n",
        "df_filled['ORIGINZIP'].head(10)"
      ],
      "metadata": {
        "id": "_tWILZVDYA8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for NONE values in DESTINATIONZIP**"
      ],
      "metadata": {
        "id": "W7eHIz3MBPhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled['DESTINATIONZIP'].head(10)"
      ],
      "metadata": {
        "id": "vScIWnklYwHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing None values**"
      ],
      "metadata": {
        "id": "9i2PbtFEBUBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df_filled.dropna(subset=['DESTINATIONZIP'])\n",
        "df_filled = df_filled[~df_filled['DESTINATIONZIP'].eq('')]\n",
        "df_filled['ORIGINZIP'].head(10)"
      ],
      "metadata": {
        "id": "pmJU3DAKYo-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**k-Nearest Neighbors (k-NN) Imputer testing**"
      ],
      "metadata": {
        "id": "9xGIxA_YN2Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding out columns to impute using KNNImputer**"
      ],
      "metadata": {
        "id": "NZg9woXzB1rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/TaxiTripDataDecmeber2023.csv')\n",
        "\n",
        "# Show missing values per column\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
        "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)\n",
        "\n",
        "print(\"\\nMissing Values Summary:\")\n",
        "print(missing_df)\n",
        "\n",
        "# Drop columns with excessive missing values (>80%)\n",
        "cols_to_drop = missing_df[missing_df['Percentage'] > 80].index.tolist()\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "print(f\"\\nDropped columns due to excessive missing values: {cols_to_drop}\")\n",
        "\n",
        "# Create a copy for imputation\n",
        "df_filled = df.copy()\n",
        "\n",
        "# Convert datetime columns\n",
        "df_filled['ORIGINDATETIME_TR'] = pd.to_datetime(df_filled['ORIGINDATETIME_TR'], errors='coerce')\n",
        "df_filled['DESTINATIONDATETIME_TR'] = pd.to_datetime(df_filled['DESTINATIONDATETIME_TR'], errors='coerce')\n",
        "\n",
        "# Feature Engineering: Calculate trip duration (minutes)\n",
        "df_filled['trip_duration'] = (df_filled['DESTINATIONDATETIME_TR'] - df_filled['ORIGINDATETIME_TR']).dt.total_seconds() / 60\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_cols = df_filled.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df_filled.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Identify numeric columns for KNN imputation (5% to 50% missing)\n",
        "impute_candidates = missing_df[\n",
        "    (missing_df['Percentage'] > 5) &\n",
        "    (missing_df['Percentage'] <= 25) &\n",
        "    (missing_df.index.isin(numeric_cols))\n",
        "].index.tolist()\n",
        "\n",
        "print(f\"\\nColumns selected for KNN imputation: {impute_candidates}\")"
      ],
      "metadata": {
        "id": "Cch8IvPNN0kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing Data to pass through KNNImputer**"
      ],
      "metadata": {
        "id": "WzoF8KD4CN4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if impute_candidates:\n",
        "    # Subset the numeric columns to impute\n",
        "    df_numeric = df_filled[numeric_cols]\n",
        "\n",
        "    # Standardize the data (KNN works better with scaled data)\n",
        "    scaler = StandardScaler()\n",
        "    df_numeric_scaled = scaler.fit_transform(df_numeric)\n",
        "\n",
        "    # Apply KNN Imputer\n",
        "    imputer = KNNImputer(n_neighbors=2)\n",
        "    df_numeric_imputed = imputer.fit_transform(df_numeric_scaled)\n",
        "\n",
        "    # Inverse transform to original scale\n",
        "    df_numeric_imputed = scaler.inverse_transform(df_numeric_imputed)\n",
        "\n",
        "    # Update the dataframe\n",
        "    df_filled[numeric_cols] = df_numeric_imputed"
      ],
      "metadata": {
        "id": "MDfj8VDaOQQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imputing rest of the columns with Mean/Median**"
      ],
      "metadata": {
        "id": "ZWD9b2_XDNkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute remaining numeric columns with low missingness (<5%) using mean/median\n",
        "for col in numeric_cols:\n",
        "    if col not in impute_candidates and df_filled[col].isnull().sum() > 0:\n",
        "        if df_filled[col].skew() > 1:  # If skewed, use median\n",
        "            df_filled[col].fillna(df_filled[col].median(), inplace=True)\n",
        "        else:  # Otherwise, use mean\n",
        "            df_filled[col].fillna(df_filled[col].mean(), inplace=True)\n",
        "\n",
        "# Fill categorical missing values with mode\n",
        "for col in categorical_cols:\n",
        "    if df_filled[col].isnull().sum() > 0:\n",
        "        df_filled[col].fillna(df_filled[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\nMissing values handled successfully with KNN Imputer for selected columns!\")\n",
        "\n",
        "\n",
        "df_filled.head(5)"
      ],
      "metadata": {
        "id": "8iAtN_m0OTp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dowloading the cleaned Dataset**"
      ],
      "metadata": {
        "id": "89NzBMGxDX1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed DataFrame to a CSV file\n",
        "df_filled.to_csv('/content/TaxiTripDataDecmeber2023cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "mFN_jLiCOb7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.shape"
      ],
      "metadata": {
        "id": "m9yTNhE4Yr4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = pd.read_csv('/content/TaxiTripDataDecmeber2023cleaned.csv')"
      ],
      "metadata": {
        "id": "JF-qv98Odtcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Outliers"
      ],
      "metadata": {
        "id": "YpkRiougDpbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape before removing outliers:\", df_filled.shape)"
      ],
      "metadata": {
        "id": "XYolUy85juUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Inter-quartile range**"
      ],
      "metadata": {
        "id": "8kAByY-zDuBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['MILEAGE', 'DURATION', 'TOTALAMOUNT', 'ORIGIN_BLOCK_LATITUDE',\n",
        "    'ORIGIN_BLOCK_LONGITUDE',\n",
        "    'DESTINATION_BLOCK_LAT',\n",
        "    'DESTINATION_BLOCK_LONG']:\n",
        "    Q1 = df_filled[col].quantile(0.25)\n",
        "    Q3 = df_filled[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df_filled = df_filled[(df_filled[col] >= lower_bound) & (df_filled[col] <= upper_bound)]\n",
        "\n",
        "print(\"Shape after removing outliers:\", df_filled.shape)"
      ],
      "metadata": {
        "id": "BrjqOmWMafLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.to_csv('/content/TaxiTripDataDecmeber2023cleaned_outlier_removed.csv', index=False)"
      ],
      "metadata": {
        "id": "uL_nuOKEirUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = pd.read_csv('/content/TaxiTripDataDecmeber2023cleaned_outlier_removed.csv')"
      ],
      "metadata": {
        "id": "0vKdahmkNipN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing further cleaning of Data to make it more efficient**"
      ],
      "metadata": {
        "id": "zzwPr1S0EI7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Clean ORIGINCITY\n",
        "print(\"\\nCleaning ORIGINCITY...\")\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].str.strip().str.title()\n",
        "\n",
        "# Replace invalid values with NaN\n",
        "invalid_values = [\"???\", \"-\", \"None\", \"Unknown\", \"Na\", \"106\", \"101\", \"Suite #203\", \"UNKNOWN\", \"- \"]\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].replace(invalid_values, np.nan)\n",
        "\n",
        "# Apply name corrections\n",
        "name_corrections = {\n",
        "    \"Washington Dc\": \"Washington\",\n",
        "    \"Mclean\": \"McLean\",\n",
        "    \"Oxon Hill Md\": \"Oxon Hill\",\n",
        "    \"District Heights Md\": \"District Heights\",\n",
        "    \"Falls Church Va\": \"Falls Church\",\n",
        "    \"Suitland-Silver Hill\": \"Suitland\",\n",
        "    \"New Carrollton Md\": \"New Carrollton\"\n",
        "}\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].replace(name_corrections)\n",
        "\n",
        "# Remove rows with full addresses\n",
        "df_filled = df_filled[~df_filled['ORIGINCITY'].str.contains(r'\\d', na=False)]\n",
        "\n",
        "# Fill NaN with mode\n",
        "df_filled['ORIGINCITY'] = df_filled['ORIGINCITY'].fillna(df_filled['ORIGINCITY'].mode()[0])\n",
        "\n",
        "# Check results\n",
        "print(\"Unique values in ORIGINCITY after cleaning:\")\n",
        "print(df_filled['ORIGINCITY'].unique())\n",
        "print(\"Count of '-' in ORIGINCITY:\", (df_filled['ORIGINCITY'] == \"-\").sum())\n",
        "print(\"Count of '???' in ORIGINCITY:\", (df_filled['ORIGINCITY'] == \"???\").sum())\n",
        "\n",
        "# Step 2: Clean DESTINATIONCITY\n",
        "print(\"\\nCleaning DESTINATIONCITY...\")\n",
        "df_filled['DESTINATIONCITY'] = df_filled['DESTINATIONCITY'].str.strip().str.title()\n",
        "\n",
        "# Replace invalid values\n",
        "invalid_values = [\"Unknown\", \"-\", \"???\", \"None\", \"UNKNOWN\", \"- \"]\n",
        "df_filled['DESTINATIONCITY'] = df_filled['DESTINATIONCITY'].replace(invalid_values, \"Missing\")\n",
        "\n",
        "# Check results\n",
        "print(\"Unique values in DESTINATIONCITY after cleaning:\")\n",
        "print(df_filled['DESTINATIONCITY'].unique())\n",
        "print(\"Count of '-' in DESTINATIONCITY:\", (df_filled['DESTINATIONCITY'] == \"-\").sum())\n",
        "print(\"Count of '???' in DESTINATIONCITY:\", (df_filled['DESTINATIONCITY'] == \"???\").sum())\n",
        "print(\"Count of 'UNKNOWN' in DESTINATIONCITY:\", (df_filled['DESTINATIONCITY'] == \"UNKNOWN\").sum())\n",
        "\n",
        "# Step 3: Clean ORIGINSTATE and DESTINATIONSTATE\n",
        "print(\"\\nCleaning ORIGINSTATE and DESTINATIONSTATE...\")\n",
        "valid_states = {\n",
        "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA',\n",
        "    'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
        "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT',\n",
        "    'VA', 'WA', 'WV', 'WI', 'WY', 'DC'\n",
        "}\n",
        "\n",
        "# Clean ORIGINSTATE\n",
        "df_filled['ORIGINSTATE'] = df_filled['ORIGINSTATE'].str.strip()\n",
        "invalid_values = [\"- \", \"Na\", \"None\", \"Unknown\", \"\", \"nan\", \"-\"]\n",
        "df_filled['ORIGINSTATE'] = df_filled['ORIGINSTATE'].replace(invalid_values, np.nan)\n",
        "df_filled['ORIGINSTATE'] = df_filled['ORIGINSTATE'].apply(lambda x: x if x in valid_states else np.nan)\n",
        "df_filled['ORIGINSTATE'] = df_filled['ORIGINSTATE'].fillna(df_filled['ORIGINSTATE'].mode()[0])\n",
        "\n",
        "# Clean DESTINATIONSTATE\n",
        "df_filled['DESTINATIONSTATE'] = df_filled['DESTINATIONSTATE'].str.strip()\n",
        "df_filled['DESTINATIONSTATE'] = df_filled['DESTINATIONSTATE'].replace(invalid_values, np.nan)\n",
        "df_filled['DESTINATIONSTATE'] = df_filled['DESTINATIONSTATE'].apply(lambda x: x if x in valid_states else np.nan)\n",
        "df_filled['DESTINATIONSTATE'] = df_filled['DESTINATIONSTATE'].fillna(df_filled['DESTINATIONSTATE'].mode()[0])\n",
        "\n",
        "# Check results\n",
        "print(\"Unique values in ORIGINSTATE after cleaning:\")\n",
        "print(df_filled['ORIGINSTATE'].unique())\n",
        "print(\"Count of '-' in ORIGINSTATE:\", (df_filled['ORIGINSTATE'] == \"-\").sum())\n",
        "\n",
        "print(\"Unique values in DESTINATIONSTATE after cleaning:\")\n",
        "print(df_filled['DESTINATIONSTATE'].unique())\n",
        "print(\"Count of '-' in DESTINATIONSTATE:\", (df_filled['DESTINATIONSTATE'] == \"-\").sum())"
      ],
      "metadata": {
        "id": "vNrU0mktne3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "G8mDDxf_EQY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Ensure datetime format\n",
        "df_filled['ORIGINDATETIME_TR'] = pd.to_datetime(df_filled['ORIGINDATETIME_TR'], errors='coerce')\n",
        "\n",
        "# Basic Information\n",
        "print(\"=== Basic Information ===\")\n",
        "print(\"\\nDataset Shape:\", df_filled.shape)\n",
        "print(\"\\nColumn Data Types:\")\n",
        "print(df_filled.dtypes)\n",
        "print(\"\\nMissing Values After Cleaning:\")\n",
        "print(df_filled.isnull().sum())\n",
        "\n",
        "# Numeric & Categorical Summary\n",
        "print(\"\\n=== Summary Statistics for Numeric Columns ===\")\n",
        "numeric_cols = ['DURATION', 'MILEAGE', 'TOTALAMOUNT',\n",
        "                'ORIGIN_BLOCK_LATITUDE', 'ORIGIN_BLOCK_LONGITUDE',\n",
        "                'DESTINATION_BLOCK_LAT', 'DESTINATION_BLOCK_LONG']\n",
        "numeric_cols = [col for col in numeric_cols if col in df_filled.columns]\n",
        "print(df_filled[numeric_cols].describe())\n",
        "\n",
        "print(\"\\n=== Summary for Categorical Columns ===\")\n",
        "categorical_cols = ['ORIGINCITY', 'DESTINATIONCITY', 'ORIGINSTATE', 'DESTINATIONSTATE', 'AIRPORT']\n",
        "categorical_cols = [col for col in categorical_cols if col in df_filled.columns]\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\nValue Counts for {col}:\")\n",
        "    print(df_filled[col].value_counts().head(10))\n",
        "\n",
        "# Create plot directory\n",
        "if not os.path.exists('eda_plots'):\n",
        "    os.makedirs('eda_plots')\n",
        "\n",
        "# Histograms\n",
        "print(\"\\nGenerating Histograms...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(df_filled[col], bins=30, kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    if i >= 9:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplots\n",
        "print(\"\\nGenerating Boxplots...\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(y=df_filled[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    if i >= 9:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation Matrix\n",
        "print(\"\\nGenerating Correlation Matrix...\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "corr_matrix = df_filled[numeric_cols].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlation Matrix of Numeric Features')\n",
        "plt.show()\n",
        "\n",
        "# Remove outliers\n",
        "print(\"Shape before removing outliers:\", df_filled.shape)\n",
        "\n",
        "# IQR for regular numeric columns (excluding trip_duration)\n",
        "for col in ['MILEAGE', 'DURATION', 'TOTALAMOUNT',\n",
        "            'ORIGIN_BLOCK_LATITUDE', 'ORIGIN_BLOCK_LONGITUDE',\n",
        "            'DESTINATION_BLOCK_LAT', 'DESTINATION_BLOCK_LONG']:\n",
        "    Q1 = df_filled[col].quantile(0.25)\n",
        "    Q3 = df_filled[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df_filled = df_filled[(df_filled[col] >= lower_bound) & (df_filled[col] <= upper_bound)]\n",
        "\n",
        "# Custom rule for trip_duration (1 to 1440 minutes)\n",
        "if 'trip_duration' in df_filled.columns:\n",
        "    df_filled = df_filled[(df_filled['trip_duration'] > 1) & (df_filled['trip_duration'] < 1440)]\n",
        "\n",
        "print(\"Shape after removing outliers:\", df_filled.shape)\n",
        "\n",
        "# Scatter plot: trip_duration vs TOTALAMOUNT\n",
        "print(\"\\nGenerating Scatter Plots...\")\n",
        "if 'trip_duration' in df_filled.columns and 'TOTALAMOUNT' in df_filled.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x='trip_duration', y='TOTALAMOUNT', data=df_filled)\n",
        "    plt.title('Trip Duration vs Total Amount')\n",
        "    plt.xlabel('Trip Duration (minutes)')\n",
        "    plt.ylabel('Total Amount ($)')\n",
        "    plt.show()\n",
        "\n",
        "# Scatter plot: MILEAGE vs TOTALAMOUNT\n",
        "if 'MILEAGE' in df_filled.columns and 'TOTALAMOUNT' in df_filled.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x='MILEAGE', y='TOTALAMOUNT', data=df_filled)\n",
        "    plt.title('Mileage vs Total Amount')\n",
        "    plt.xlabel('Mileage (miles)')\n",
        "    plt.ylabel('Total Amount ($)')\n",
        "    plt.show()\n",
        "\n",
        "# Temporal Analysis\n",
        "if 'ORIGINDATETIME_TR' in df_filled.columns:\n",
        "    print(\"\\nGenerating Temporal Analysis...\")\n",
        "    if df_filled['ORIGINDATETIME_TR'].dtype != 'datetime64[ns]':\n",
        "        df_filled['ORIGINDATETIME_TR'] = pd.to_datetime(df_filled['ORIGINDATETIME_TR'], errors='coerce')\n",
        "\n",
        "    # Drop invalid dates\n",
        "    initial_shape = df_filled.shape\n",
        "    df_filled = df_filled.dropna(subset=['ORIGINDATETIME_TR'])\n",
        "    print(f\"Dropped {initial_shape[0] - df_filled.shape[0]} rows with NaT in ORIGINDATETIME_TR\")\n",
        "\n",
        "    # Extract hour/day\n",
        "    df_filled['hour_of_day'] = df_filled['ORIGINDATETIME_TR'].dt.hour\n",
        "    df_filled['day_of_week'] = df_filled['ORIGINDATETIME_TR'].dt.day_name()\n",
        "\n",
        "    # Trips by Hour\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x='hour_of_day', data=df_filled)\n",
        "    plt.title('Number of Trips by Hour of Day')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Number of Trips')\n",
        "    plt.show()\n",
        "\n",
        "    # Trips by Day\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x='day_of_week', data=df_filled, order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "    plt.title('Number of Trips by Day of Week')\n",
        "    plt.xlabel('Day of Week')\n",
        "    plt.ylabel('Number of Trips')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nORIGINDATETIME_TR not found in DataFrame. Skipping temporal analysis.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wm5QZF5bNud-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "df_filled = df_filled.dropna(subset=['ORIGIN_BLOCK_LATITUDE', 'ORIGIN_BLOCK_LONGITUDE'])\n",
        "\n",
        "map_center = [df_filled['ORIGIN_BLOCK_LATITUDE'].mean(), df_filled['ORIGIN_BLOCK_LONGITUDE'].mean()]\n",
        "m = folium.Map(location=map_center, zoom_start=11)\n",
        "\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "for idx, row in df_filled.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['ORIGIN_BLOCK_LATITUDE'], row['ORIGIN_BLOCK_LONGITUDE']],\n",
        "        radius=4,\n",
        "        color='red',\n",
        "        fill=True,\n",
        "        fill_color='red',\n",
        "        fill_opacity=0.6,\n",
        "        popup=f\"Origin: ({row['ORIGIN_BLOCK_LATITUDE']:.5f}, {row['ORIGIN_BLOCK_LONGITUDE']:.5f})\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "RhroGWvOl8eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "df_filled = df_filled.dropna(subset=['DESTINATION_BLOCK_LAT', 'DESTINATION_BLOCK_LONG'])\n",
        "\n",
        "map_center = [df_filled['DESTINATION_BLOCK_LAT'].mean(), df_filled['DESTINATION_BLOCK_LONG'].mean()]\n",
        "m = folium.Map(location=map_center, zoom_start=11)\n",
        "\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "for idx, row in df_filled.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['DESTINATION_BLOCK_LAT'], row['DESTINATION_BLOCK_LONG']],\n",
        "        radius=4,\n",
        "        color='green',\n",
        "        fill=True,\n",
        "        fill_color='green',\n",
        "        fill_opacity=0.6,\n",
        "        popup=f\"Destination: ({row['DESTINATION_BLOCK_LAT']:.5f}, {row['DESTINATION_BLOCK_LONG']:.5f})\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m"
      ],
      "metadata": {
        "id": "NvnQSBtrmo8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled.columns"
      ],
      "metadata": {
        "id": "RwXu7pzqoL7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning**"
      ],
      "metadata": {
        "id": "fnVT5QntO-s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What are the most determining variables for the total fare of rides?"
      ],
      "metadata": {
        "id": "HhGZWyLJbhbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "numeric_cols = df_filled.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Removing target from features list if present\n",
        "target = 'TOTALAMOUNT'\n",
        "if target in numeric_cols:\n",
        "    numeric_cols.remove(target)\n",
        "\n",
        "# Prepare data\n",
        "X = df_filled[numeric_cols]\n",
        "y = df_filled[target]\n",
        "\n",
        "# Drop missing values\n",
        "df_model = pd.concat([X, y], axis=1).dropna()\n",
        "X = df_model[numeric_cols]\n",
        "y = df_model[target]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Take Top 5 Features\n",
        "top_features = feature_importance_df.head(5)\n",
        "\n",
        "# Plot horizontally\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(\n",
        "    y='Importance',\n",
        "    x='Feature',\n",
        "    data=top_features,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title('Top 5 Feature Importance for Predicting Total Fare (Random Forest)', fontsize=14)\n",
        "plt.ylabel('Importance', fontsize=12)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Print the top 5 feature importance table\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "id": "c9KzhyXpvOHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. How does the ride fare change with different cities, states, and times?"
      ],
      "metadata": {
        "id": "vuqSM_0NbzNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **By Origin City**"
      ],
      "metadata": {
        "id": "Lo-_GiH2cuDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Fare by Origin City\n",
        "city_avg = df_filled.groupby('ORIGINCITY')['TOTALAMOUNT'].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "city_avg.plot(kind='bar')\n",
        "plt.title('Average Fare by Top 10 Origin Cities')\n",
        "plt.ylabel('Average Total Fare ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K9JgxNG9bpdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**by Origin State**"
      ],
      "metadata": {
        "id": "xs17hA_mc422"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_avg = df_filled.groupby('ORIGINSTATE')['TOTALAMOUNT'].mean().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "state_avg.plot(kind='bar')\n",
        "plt.title('Average Fare by Origin States')\n",
        "plt.ylabel('Average Total Fare ($)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FAZpx10Db3QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**by Hour of Day**"
      ],
      "metadata": {
        "id": "XTGlhS-zc-Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fare by Hour of Day\n",
        "hourly_avg = df_filled.groupby('hour_of_day')['TOTALAMOUNT'].mean()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "hourly_avg.plot(kind='line', marker='o')\n",
        "plt.title('Average Fare by Hour of Day')\n",
        "plt.xlabel('Pickup Hour')\n",
        "plt.ylabel('Average Total Fare ($)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cvT570UpdCYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**by Day of Week**"
      ],
      "metadata": {
        "id": "RiexJ1AmdGYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fare by Day of Week\n",
        "dow_avg = df_filled.groupby('day_of_week')['TOTALAMOUNT'].mean()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "dow_avg.plot(kind='line', marker='s')\n",
        "plt.title('Average Fare by Day of Week')\n",
        "plt.xlabel('Day of Week (0=Monday)')\n",
        "plt.ylabel('Average Total Fare ($)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ppj9w-dxdIqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How does trip duration affect the total fare, and is it linear across time?\n",
        "\n"
      ],
      "metadata": {
        "id": "olCbT_NHdUqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "\n",
        "df_plot = df_filled[df_filled['trip_duration'] <= 500]\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(x='trip_duration', y='TOTALAMOUNT', data=df_plot, alpha=0.3)\n",
        "\n",
        "# LOWESS smoothing\n",
        "lowess = sm.nonparametric.lowess\n",
        "z = lowess(df_plot['TOTALAMOUNT'], df_plot['trip_duration'], frac=0.2)  # frac controls smoothing\n",
        "plt.plot(z[:, 0], z[:, 1], color='red', label='LOWESS Smoothed Trend', lw=2)\n",
        "\n",
        "# Correlation\n",
        "corr = df_plot['trip_duration'].corr(df_plot['TOTALAMOUNT'])\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(f'Trip Duration vs Total Fare (Smoothed Trend)\\nCorrelation: {corr:.2f}', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Trip Duration (minutes)', fontsize=14)\n",
        "plt.ylabel('Total Fare ($)', fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TGXW5eol1V0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the most frequent payment methods, and is there any relationship with the amount of gratuity?"
      ],
      "metadata": {
        "id": "Y8FFSeDTfGuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping based on Source Data Readme file\n",
        "\n",
        "payment_type_mapping = {\n",
        "    1.0: 'Credit',\n",
        "    2.0: 'Cash',\n",
        "    3.0: 'EHail',\n",
        "    4.0: 'Other',\n",
        "    6.0: 'Uber Credit'\n",
        "}\n",
        "# Apply payment type labels\n",
        "df_filled['PAYMENTTYPE_LABEL'] = df_filled['PAYMENTTYPE'].map(payment_type_mapping)\n",
        "\n",
        "# Checking payment type frequency\n",
        "payment_counts = df_filled['PAYMENTTYPE_LABEL'].value_counts()\n",
        "print(\"Payment Type Counts:\\n\", payment_counts)\n",
        "\n",
        "# Bar plot of payment type frequency\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(data=df_filled, x='PAYMENTTYPE_LABEL', order=payment_counts.index, palette='viridis') # Changed 'PAYMENT_TYPE' to 'PAYMENTTYPE'\n",
        "plt.title('Frequency of Payment Methods', fontsize=16)\n",
        "plt.xlabel('Payment Type', fontsize=14)\n",
        "plt.ylabel('Number of Rides', fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, axis='y')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Summary statistics\n",
        "gratuity_summary = df_filled.groupby('PAYMENTTYPE_LABEL')['GRATUITYAMOUNT'].describe() # Changed 'PAYMENT_TYPE' to 'PAYMENTTYPE'\n",
        "print(\"\\nGratuity Summary by Payment Type:\\n\", gratuity_summary)"
      ],
      "metadata": {
        "id": "xpEVlrH94wMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict TOTALAMOUNT based on features like MILEAGE, DURATION, FAREAMOUNT, GRATUITYAMOUNT, etc?"
      ],
      "metadata": {
        "id": "gh8RXIvghuyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Features and Target\n",
        "features = ['MILEAGE', 'DURATION', 'FAREAMOUNT', 'GRATUITYAMOUNT', 'trip_duration']\n",
        "X = df_filled[features]\n",
        "y = df_filled['TOTALAMOUNT']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Regression Accuracy\n",
        "def regression_accuracy(y_true, y_pred, tolerance=0.10):\n",
        "    within_tolerance = np.abs(y_true - y_pred) <= (tolerance * np.abs(y_true))\n",
        "    return np.mean(within_tolerance) * 100\n",
        "\n",
        "# Model Training and Evaluation Function\n",
        "def train_and_evaluate(model, model_name):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "    print(f\"R² Score: {r2_score(y_test, y_pred):.2f}\")\n",
        "    print(f\"Accuracy (within 10%): {regression_accuracy(y_test, y_pred):.2f}%\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
        "train_and_evaluate(rf, \"Random Forest\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, random_state=42)\n",
        "train_and_evaluate(gb, \"Gradient Boosting\")\n"
      ],
      "metadata": {
        "id": "nG_cDeKWfr5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}